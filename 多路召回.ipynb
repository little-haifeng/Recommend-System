{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import os, math, warnings, math, pickle\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import collections\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './download/'\n",
    "save_path = './temp_results/'\n",
    "# 做召回评估的一个标志, 如果不进行评估就是直接使用全量数据进行召回\n",
    "metric_recall = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    \"\"\"\n",
    "        训练集中采样一部分数据调试\n",
    "        data_path: 原数据的存储路径\n",
    "        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False) \n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_info_df(data_path):\n",
    "    item_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    \n",
    "    # 为了方便与训练集中的click_article_id拼接，需要把article_id修改成click_article_id\n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    \n",
    "    return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_emb_dict(data_path):\n",
    "    item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "    \n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "    # 进行归一化\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "    pickle.dump(item_emb_dict, open(save_path + 'item_content_emb.pkl', 'wb'))\n",
    "    \n",
    "    return item_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df = get_all_click_sample(data_path,sample_nums=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_info_df = get_item_info_df(data_path)\n",
    "item_emb_dict = get_item_emb_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据点击时间获取用户的点击文章序列   {user1: [(item1, time1), (item2, time2)..]...}\n",
    "def get_user_item_time(click_df):\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    \n",
    "    def make_item_time_pair(df):\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "    \n",
    "    user_item_time_df = click_df.groupby('user_id')['click_article_id', 'click_timestamp'].apply(lambda x: make_item_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "    \n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据时间获取商品被点击的用户序列  {item1: [(user1, time1), (user2, time2)...]...}\n",
    "# 这里的时间是用户点击当前商品的时间，好像没有直接的关系。\n",
    "def get_item_user_time_dict(click_df):\n",
    "    def make_user_time_pair(df):\n",
    "        return list(zip(df['user_id'], df['click_timestamp']))\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    item_user_time_df = click_df.groupby('click_article_id')['user_id', 'click_timestamp'].apply(lambda x: make_user_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'user_time_list'})\n",
    "    \n",
    "    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "    return item_user_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前数据的历史点击和最后一次点击\n",
    "def get_hist_and_last_click(all_click):\n",
    "    \n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文章id对应的基本属性，保存成字典的形式，方便后面召回阶段，冷启动阶段直接使用\n",
    "def get_item_info_dict(item_info_df):\n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "    \n",
    "    return item_type_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_hist_item_info_dict(all_click):\n",
    "    \n",
    "    # 获取user_id对应的用户历史点击文章类型的集合字典\n",
    "    user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "    user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户点击文章的集合\n",
    "    user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户历史点击的文章的平均字数字典\n",
    "    user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "    \n",
    "    # 获取user_id对应的用户最后一次点击的文章的创建时间\n",
    "    all_click_ = all_click.sort_values('click_timestamp')\n",
    "    user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    \n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "    \n",
    "    return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取近期点击最多的文章\n",
    "def get_item_topk_click(click_df, k):\n",
    "    topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "    return topk_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文章的属性信息，保存成字典的形式方便查询\n",
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'youtubednn_usercf_recall': {}, \n",
    "                           'cold_start_recall': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取最后一次点击作为召回评估，如果不需要做召回评估直接使用全量的训练集进行召回(线下验证模型)\n",
    "# 如果不是召回评估，直接使用全量数据进行召回，不用将最后一次提取出来\n",
    "trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依次评估召回的前10, 20, 30, 40, 50个文章中的击中率\n",
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=5):\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "    user_num = len(user_recall_items_dict)\n",
    "    \n",
    "    for k in range(10, topk+1, 10):\n",
    "        hit_num = 0\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            # 获取前k个召回的结果\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1\n",
    "        \n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "        print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df, item_created_time_dict):\n",
    "    \"\"\"\n",
    "        文章与文章之间的相似性矩阵计算\n",
    "        :param df: 数据表\n",
    "        :item_created_time_dict:  文章创建时间的字典\n",
    "        return : 文章与文章的相似性矩阵\n",
    "        \n",
    "        思路: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则\n",
    "    \"\"\"\n",
    "    \n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # 计算物品相似度\n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        # 在基于商品的协同过滤优化的时候可以考虑时间因素\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                    \n",
    "                # 考虑文章的正向顺序点击和反向顺序点击    \n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                # 位置信息权重，其中的参数可以调节\n",
    "                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # 点击时间权重，其中的参数可以调节\n",
    "                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                # 两篇文章创建时间的权重，其中的参数可以调节\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                # 考虑多种因素的权重计算最终的文章之间的相似度\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "                \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_activate_degree_dict(all_click_df):\n",
    "    all_click_df_ = all_click_df.groupby('user_id')['click_article_id'].count().reset_index()\n",
    "    \n",
    "    # 用户活跃度归一化\n",
    "    mm = MinMaxScaler()\n",
    "    all_click_df_['click_article_id'] = mm.fit_transform(all_click_df_[['click_article_id']])\n",
    "    user_activate_degree_dict = dict(zip(all_click_df_['user_id'], all_click_df_['click_article_id']))\n",
    "    \n",
    "    return user_activate_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usercf_sim(all_click_df, user_activate_degree_dict):\n",
    "    \"\"\"\n",
    "        用户相似性矩阵计算\n",
    "        :param all_click_df: 数据表\n",
    "        :param user_activate_degree_dict: 用户活跃度的字典\n",
    "        return 用户相似性矩阵\n",
    "        \n",
    "        思路: 基于用户的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则\n",
    "    \"\"\"\n",
    "    item_user_time_dict = get_item_user_time_dict(all_click_df)\n",
    "    \n",
    "    u2u_sim = {}\n",
    "    user_cnt = defaultdict(int)\n",
    "    for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "        for u, click_time in user_time_list:\n",
    "            user_cnt[u] += 1\n",
    "            u2u_sim.setdefault(u, {})\n",
    "            for v, click_time in user_time_list:\n",
    "                u2u_sim[u].setdefault(v, 0)\n",
    "                if u == v:\n",
    "                    continue\n",
    "                # 用户平均活跃度作为活跃度的权重，这里的式子也可以改善\n",
    "                activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])   \n",
    "                u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)\n",
    "    \n",
    "    u2u_sim_ = u2u_sim.copy()\n",
    "    for u, related_users in u2u_sim.items():\n",
    "        for v, wij in related_users.items():\n",
    "            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(u2u_sim_, open(save_path + 'usercf_u2u_sim.pkl', 'wb'))\n",
    "\n",
    "    return u2u_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于usercf计算时候太耗费内存了，这里就不直接运行了\n",
    "# 如果是采样的话，是可以运行的\n",
    "user_activate_degree_dict = get_user_activate_degree_dict(all_click_df)\n",
    "u2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_df = pd.read_csv(data_path + '/articles_emb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df['article_id']))\n",
    "    \n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n",
    "    # 向量进行单位化\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "    \n",
    "    # 建立faiss索引\n",
    "    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])\n",
    "    item_index.add(item_emb_np)\n",
    "    # 相似度查询，给每个索引位置上的向量返回topk个item以及相似度\n",
    "    sim, idx = item_index.search(item_emb_np, 10) # 返回的是列表\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364047it [00:18, 19408.49it/s]\n"
     ]
    }
   ],
   "source": [
    " # 将向量检索的结果保存成原始id的对应关系\n",
    "    item_sim_dict = collections.defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n",
    "        target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "        # 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]): \n",
    "            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "     # 保存i2i相似度矩阵\n",
    "    pickle.dump(item_sim_dict, open(save_path + 'emb_i2i_sim.pkl', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
